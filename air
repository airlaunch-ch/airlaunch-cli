#!/bin/bash

# Copyright 2022 Airlaunch GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## Default Config
export AIRFLOW_DAGS_PATH=${AIRFLOW_DAGS_PATH=$(pwd)}
export AIRFLOW_HOME=${AIRFLOW_HOME:=$(pwd)/airflow-home}
export AIRFLOW_VERSION=${AIRFLOW_VERSION:=2.2.4}
export AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS:=amazon,azure,google,postgres}
export AIRFLOW_REQUIREMENTS_FILE=${AIRFLOW_REQUIREMENTS_FILE:=$AIRFLOW_DAGS_PATH/requirements.txt}
export AIRFLOW_VENV_NAME=${AIRFLOW_VENV_NAME:=venv}
export AIRFLOW_CONNECTIONS_FILE=${AIRFLOW_CONNECTIONS_FILE:=$AIRFLOW_DAGS_PATH/connections.yaml}

## Airflow Config
export AIRFLOW__CORE__LOAD_EXAMPLES=False
export AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
export AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
export AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=10

# trap ctrl-c and call ctrl_c()
trap ctrl_c INT

function ctrl_c() {
    echo "Terminating Airflow components"
    pkill -P $$
    rm $AIRFLOW_HOME/airflow.pid
    sleep 2
    exit
}

function stop_airflow() {
  echo "stopping airflow"
  pkill --pidfile $AIRFLOW_HOME/airflow.pid
}

function activate_venv(){
  source_env
  if [ ! -f "$AIRFLOW_VENV_NAME/bin/activate" ]; then
    echo "No virtual environment named '$AIRFLOW_VENV_NAME' found. Run 'air env init' to bootstrap one."
    exit 0
  fi
  source $AIRFLOW_VENV_NAME/bin/activate
}

function source_env(){
    if [ ! -f ".env" ]; then
      echo "no .env file found, creating the default file."
      echo "# Use these environment variables to configure your enviornment. Default should work fine

# Airflow version to be installed
export AIRFLOW_VERSION=$AIRFLOW_VERSION

# Airflow extras to install. Check https://airflow.apache.org/docs/apache-airflow/2.0.0/extra-packages-ref.html for available extras
# export AIRFLOW_EXTRAS=apache.atlas,apache.beam,apache.cassandra,apache.drill,apache.druid,apache.hdfs,apache.hive,apache.kylin,apache.livy,apache.pig,apache.pinot,apache.spark,apache.sqoop,apache.webhdfs,airbyte,alibaba,amazon,asana,azure,cloudant,databricks,datadog,dingding,discord,facebook,google,hashicorp,jira,opsgenie,pagerduty,plexus,qubole,salesforce,sendgrid,segment,slack,snowflake,tableau,telegram,vertica,yandex,zendesk
export AIRFLOW_EXTRAS=$AIRFLOW_EXTRAS

# Path to the Airflow DAG files
# WARNING: # dags path can be anywhere on your system, but make sure that airflow-home is not a subfolder of your dags folder as this confuses the airflow scheduler
export AIRFLOW_DAGS_PATH=$AIRFLOW_DAGS_PATH

# Name of the python venv that is created
export AIRFLOW_VENV_NAME=$AIRFLOW_VENV_NAME

# Path to the pip requirements file. These will be instaled automatically on startup
export AIRFLOW_REQUIREMENTS_FILE=$AIRFLOW_REQUIREMENTS_FILE

# Path to the airflow home folder. This is the folder where metadata and airflow internal config files are stored. Usually, there is no need for manual intervention in this folder
# WARNING: Make sure that airflow-home is not a subfolder of your dags folder as this confuses the airflow scheduler
export AIRFLOW_HOME=$AIRFLOW_HOME

# Path to an optional yaml file holding connection definitions in yaml format. Use the format as defined here: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#exporting-connections-from-the-cli 
# You can also use the web interface to create connections and generate this file later on using our export function. 
# This file will be imported on startup and when the comman air load is run 
export AIRFLOW_CONNECTIONS_FILE=$AIRFLOW_CONNECTIONS_FILE
" > .env
    fi
    source .env
    export AIRFLOW__CORE__DAGS_FOLDER=$AIRFLOW_DAGS_PATH
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:///${AIRFLOW_HOME}/airflow.db
}

function create_airflowignore(){
  airflow_home_rel=${AIRFLOW_HOME#"$AIRFLOW_DAGS_PATH/"}
  if [ ! -f "$AIRFLOW_DAGS_PATH/.airflowignore" ]; then
    echo "# default .airflowignore file making sure the scheduler ignores the venv and airflow home folder. 
# The Airflow Scheduler gets confused sometines if ir processes the airflow-home folder
${airflow_home_rel}/
${AIRFLOW_VENV_NAME}/

# You can add your own ignore clauses below
">$AIRFLOW_DAGS_PATH/.airflowignore
  fi
}

function install_requirements(){
    source_env
    activate_venv
    if [ -f "$AIRFLOW_REQUIREMENTS_FILE" ]; then
      PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
      CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
      pip install -r $AIRFLOW_REQUIREMENTS_FILE --constraint "${CONSTRAINT_URL}"
    fi
}

function init(){
  echo "initializing environment"
  source_env
  virtualenv $AIRFLOW_VENV_NAME
  activate_venv
  PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
  CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
  pip install "apache-airflow[${AIRFLOW_EXTRAS}]"==${AIRFLOW_VERSION} pyyaml --constraint "${CONSTRAINT_URL}"
  create_airflowignore
  airflow db init
  disable_auth
  install_requirements
  load_connections
}

function disable_auth(){
  if ! grep -Fxq "AUTH_ROLE_PUBLIC = 'Admin'" $AIRFLOW_HOME/webserver_config.py
  then
    echo "AUTH_ROLE_PUBLIC = 'Admin'" >> $AIRFLOW_HOME/webserver_config.py
  fi
}

function load_connections(){
  source_env
  activate_venv
  drop_connections
  airflow connections import $AIRFLOW_CONNECTIONS_FILE
}

function start_verbose(){
  source_env
  activate_venv
  load_connections
  install_requirements

  airflow standalone
}

function start_detached(){
  source_env
  activate_venv
  load_connections
  install_requirements

  airflow standalone &> /dev/null &
  echo $! > $AIRFLOW_HOME/airflow.pid
  echo "started airflow enviornment"
  disown
}

function export_airflow(){
  source_env
  activate_venv
  if [ "$#" -eq 1 ]
  then
    OUTPUT_FILE=$1
  else
    OUTPUT_FILE=$AIRFLOW_CONNECTIONS_FILE
  fi
  echo "exporting connections to $OUTPUT_FILE"
  airflow connections export --format yaml $OUTPUT_FILE
}

function test_dag(){
  source_env
  activate_venv
  if [ "$#" -eq 1 ]
  then
    execution_date=$(date -I)
  else
    execution_date=$2
  fi
  airflow dags test $1 $execution_date
}

function test_task(){
  source_env
  activate_venv
  if [ "$#" -eq 2 ]
  then
    execution_date=$(date -I)
  else
    execution_date=$3
  fi
  airflow tasks test $1 $2 $execution_date
}

function delete_airflow_env(){
  source_env
  rm -r $AIRFLOW_HOME
  rm -r $AIRFLOW_VENV_NAME
}

function drop_connections(){
  IFS=''
  connections='('
  while read -r line 
  do  
    if [[ "$line" =~ ^[a-z,0-9,-,_]*:$ ]]
    then
      line=`echo $line | sed 's/://'`
      connections+="'$line',"
    fi
  done <$AIRFLOW_CONNECTIONS_FILE
  if [ ${#connections} -eq 1 ]
  then
    return 0
  fi
  connections=`echo $connections | sed 's/.$//'`
  connections+=")"
  echo "import sqlite3
conn = sqlite3.connect('{}/airflow.db'.format('$AIRFLOW_HOME'))
conn.execute(\"DELETE FROM connection WHERE conn_id IN $connections\")
conn.commit()" | python 
}

function print_help(){
  echo "
Wrapper skript to manage Apache Airflow development Enviornments in python virtual environments.
It adds the 'env' subcommand with functionality to automatically install and run airflow development environments.

Configuration: 


Script usage:

    Manage Enviornments:
        initialize airflow environment:                         air env init
        install requirements from file requirements.txt file:   air env install-requirements
        start airflow enviropnment:                             air env start ([-d] flag for detached mode)
        stop airflow environment:                               air env stop (or ctrl-c)
        restart the environment:                                air env restart
        delete the local environment:                           air env delete
        (re) load connections file:                             air env load
        export connections to file:                             air env export [optional filename]. If no filename is provided, AIRFLOW_CONNECTIONS_FILE is used
    
    Use Airflow Command Line (just replace the airflow command with the air command): 
        activate venv and run any airflow command:              air [airflow_group] [airflow_command]

Airflow Usage:
"

source_env
activate_venv 
airflow --help
}

if [ "$1" == "env" ]
then
  if [ "$2" == "init" ]
  then
    init
  elif [ "$2" == "load" ]
  then
    load_connections
  elif [ "$2" == "start" ]
  then
    if [ "$3" == "-d" ]
    then
      start_detached
    else
      start_verbose
    fi
  elif [ "$2" == "stop" ]
  then
    stop_airflow
  elif [ "$2" == "install-requirements" ]
  then
    install_requirements
  elif [ "$2" == "export" ]
  then
    shift 1
    export_airflow "$@"
  elif [ "$2" == "restart" ]
  then
    echo "restarting airflow environment"
    stop_airflow
    if [ "$3" == "-d" ]
    then
      start_detached
    else
      start_verbose
    fi
  elif [ "$2" == "delete" ]
  then
    delete_airflow_env
  elif [ "$2" == "-h" ] || [ "$2" == "--help" ] || [ "$2" == "help" ]
  then
    print_help
  else
    print_help
  fi
elif [ "$1" == "-h" ] || [ "$1" == "--help" ] || [ "$1" == "help" ] || [ "$#" -eq 0 ]
then
  print_help
else
  source_env
  activate_venv
  shift 1
  airflow "$@"
fi